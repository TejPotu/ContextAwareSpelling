{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misspelled Words: ['Accomodate', 'Aquire', 'Aficianado', 'Isle', 'Amatuer', 'Apparant', 'Artic', 'Arguement', 'Athiest', 'Belive', 'Bizzare', 'Calender', 'Carribean', 'Cemetary', 'Cheif', 'Collegue', 'Collectable', 'Columist', 'Commitee', 'Comitted', 'Concensus', 'Definately', 'Dilemna', 'Dissapoint', 'Embarras', 'Embarassed', 'Enviroment', 'Exilerate', 'Facinate', 'Florescent', 'Foriegn', 'Fourty', 'Freind', 'Gage', 'Goverment', 'Greatful', 'Happend', 'Harras', 'Horderves', 'Humourous', 'Immediatly', 'Independant', 'Jewelry', 'Judgement', 'Knowlege', 'Liesure', 'Liason', 'Lightening', 'Loose', 'Maintanance', 'Manuever', 'Medival', 'Momento', 'Millenium', 'Minature', 'Mischevious', 'Mispell', 'Nausious', 'Neccessary', 'Ocassion', 'Occured', 'Parralel', 'Pavilion', 'Perseverence', 'Phillipines', 'Playwrite', 'Privelege', 'Publically', 'Questionaire', 'Recieve', 'Recomend', 'Resistence', 'Responsability', 'Rythm', 'Sacreligious', 'Shedule', 'Sence', 'Seperate', 'Seige', 'Strenght', 'Succint', 'Supercede', 'Tatoo', 'Tendancy', 'Threshhold', 'Tollerance', 'Truely', 'Unforseen', 'Unecessary', 'Untill', 'Vacum', 'Viscious', 'Whether', 'Wether', 'Wich', 'Wierd', 'Whereever', 'Writting', 'Yatch', 'Zealos']\n",
      "Correct Spellings: ['Accommodate', 'Acquire', 'Aficionado', 'Aisle', 'Amateur', 'Apparent', 'Arctic', 'Argument', 'Atheist', 'Believe', 'Bizarre', 'Calendar', 'Caribbean', 'Cemetery', 'Chief', 'Colleague', 'Collectible', 'Columnist', 'Committee', 'Committed', 'Consensus', 'Definitely', 'Dilemma', 'Disappoint', 'Embarrass', 'Embarrassed', 'Environment', 'Exhilarate', 'Fascinate', 'Fluorescent', 'Foreign', 'Forty', 'Friend', 'Gauge', 'Government', 'Grateful', 'Happened', 'Harass', \"Hors d'oeuvre\", 'Humorous', 'Immediately', 'Independent', 'Jewellery', 'Judgment', 'Knowledge', 'Leisure', 'Liaison', 'Lightning', 'Lose', 'Maintenance', 'Manoeuvre', 'Medieval', 'Memento', 'Millennium', 'Miniature', 'Mischievous', 'Misspell', 'Nauseous', 'Necessary', 'Occasion', 'Occurred', 'Parallel', 'Pavillion', 'Perseverance', 'Philippines', 'Playwright', 'Privilege', 'Publicly', 'Questionnaire', 'Receive', 'Recommend', 'Resistance', 'Responsibility', 'Rhythm', 'Sacrilegious', 'Schedule', 'Sense', 'Separate', 'Siege', 'Strength', 'Succinct', 'Supersede', 'Tattoo', 'Tendency', 'Threshold', 'Tolerance', 'Truly', 'Unforeseen', 'Unnecessary', 'Until', 'Vacuum', 'Vicious', 'Weather', 'Whether', 'Which', 'Weird', 'Wherever', 'Writing', 'Yacht', 'Zealous']\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty lists to store misspelled words and their correct spellings\n",
    "misspelled_words = []\n",
    "correct_spellings = []\n",
    "\n",
    "# Read the contents of the text file\n",
    "with open('misspelled_words.txt', 'r') as file:\n",
    "    # Iterate through each line in the file\n",
    "    for line in file:\n",
    "        # Split the line into misspelled word and correct spelling\n",
    "        correct, misspelled = line.strip().split('\\t')\n",
    "        # Append misspelled word to the list\n",
    "        misspelled_words.append(misspelled)\n",
    "        # Append correct spelling to the list\n",
    "        correct_spellings.append(correct)\n",
    "\n",
    "# Print the lists to verify\n",
    "print(\"Misspelled Words:\", misspelled_words)\n",
    "print(\"Correct Spellings:\", correct_spellings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misspelled Embeddings: {'input_ids': tensor([[  101, 16222, 19506, 13701,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "Correct Embeddings: {'input_ids': tensor([[ 101, 8752,  102]]), 'token_type_ids': tensor([[0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load BERT tokenizer and model \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "model1 = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "# Initialize lists to store tokenized inputs\n",
    "misspelled_tokenized = []\n",
    "correct_tokenized = []\n",
    "\n",
    "# print the model architecture\n",
    "# print(model)\n",
    "\n",
    "def get_bert_embeddings(emb_model, words):\n",
    "    # Tokenize the words and prepare BERT input\n",
    "    encoded_input = tokenizer(words, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    # Extract embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = emb_model(**encoded_input)\n",
    "        input_embeddings = emb_model.embeddings.word_embeddings(encoded_input['input_ids'])\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    # Aggregate the embeddings for each word (simple approach)\n",
    "    embeddings = torch.mean(hidden_states, dim=1)\n",
    "    return encoded_input, input_embeddings, embeddings\n",
    "\n",
    "\n",
    "# Tokenize misspelled and correct words\n",
    "for misspelled, correct in zip(misspelled_words, correct_spellings):\n",
    "    correct_tokenized.append(get_bert_embeddings(model, correct))\n",
    "    misspelled_tokenized.append(get_bert_embeddings(model1, misspelled))\n",
    "\n",
    "# print the shape of the embeddings\n",
    "#print(\"Misspelled Embeddings Shape:\", misspelled_tokenized[0].shape)\n",
    "#print(\"Correct Embeddings Shape:\", correct_tokenized[0].shape)\n",
    "\n",
    "\n",
    "print(\"Misspelled Embeddings:\", misspelled_tokenized[0][0][:10])\n",
    "print(\"Correct Embeddings:\", correct_tokenized[0][0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misspelled Tokenized: [[14945, 316, 88111], [32, 999], [55439, 12734, 2172], [3957, 273], [6219, 266, 8977]]\n",
      "Correct Tokenized: [[14945, 316, 2658, 349], [11916, 999], [55439, 14222, 2172], [32, 41205], [6219, 11067]]\n",
      "Token Matches: [['Acc', 'om'], ['quire'], ['ado', 'Af'], [], ['Am']]\n"
     ]
    }
   ],
   "source": [
    "# Using tiktoken to extract the BPE tokens used in GPT-4\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "# Initialize lists to store tokenized inputs and their matches\n",
    "misspelled_tokenized = []\n",
    "correct_tokenized = []\n",
    "token_matches = []\n",
    "#print(enc.encode(\"hello world\"))\n",
    "for misspelled, correct in zip(misspelled_words, correct_spellings):\n",
    "    # Tokenize the misspelled word\n",
    "    misspelled_tokens = enc.encode(misspelled)\n",
    "    misspelled_tokenized.append(misspelled_tokens)\n",
    "    \n",
    "    # Tokenize the correct word\n",
    "    correct_tokens = enc.encode(correct)\n",
    "    correct_tokenized.append(correct_tokens)\n",
    "    \n",
    "    # Find matching tokens\n",
    "    matches = set(misspelled_tokens).intersection(set(correct_tokens))\n",
    "    matches = [enc.decode([token]) for token in matches]\n",
    "    token_matches.append(matches)\n",
    "\n",
    "\n",
    "# Print tokenized outputs and their matches\n",
    "print(\"Misspelled Tokenized:\", misspelled_tokenized[:5])\n",
    "print(\"Correct Tokenized:\", correct_tokenized[:5])\n",
    "print(\"Token Matches:\", token_matches[:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3041, 5372, 502, 416, 597, 2420, 345, 1549, 588, 13]\n",
      "torch.Size([768])\n",
      "tensor([-2.2211e-01, -2.4913e-02, -7.9644e-01,  1.2311e-01, -1.5142e-02,\n",
      "         2.0473e-01,  2.0380e+00, -2.8743e-01,  8.8317e-02, -1.9279e-01,\n",
      "         2.0039e-01, -2.7463e-01, -2.6310e-01,  7.0118e-02, -2.6523e-01,\n",
      "        -2.2119e-01, -7.9613e-02, -4.5220e-01,  5.0428e-01,  2.4738e-01,\n",
      "        -5.8950e-02, -1.6384e-01, -1.2509e-01,  4.8210e-02,  1.5231e-01,\n",
      "         3.1702e-02, -5.6753e-01, -5.0967e-02,  2.3500e-01,  3.3692e-01,\n",
      "        -9.4491e-02, -1.0552e-01, -1.3025e-01, -4.3243e-01, -3.2330e-01,\n",
      "        -4.9765e-01,  6.4552e+01,  2.5136e-01,  2.0790e-01,  3.4858e-01,\n",
      "        -3.4390e-01,  2.9791e-01,  1.8456e-01, -1.1489e-01,  2.0997e-02,\n",
      "        -2.6759e-01, -9.1552e-02, -6.4487e-01, -1.0714e-01,  1.2296e+00,\n",
      "         9.4861e-02,  3.1005e-01, -1.8994e-01,  2.0349e-01,  1.3099e-02,\n",
      "         5.5352e-01, -1.7095e-02,  7.8032e-02,  2.0283e-02, -1.4592e-01,\n",
      "         1.5231e-01,  8.5904e-02,  4.7031e-02, -7.2096e-02, -1.1056e+00,\n",
      "         1.1989e-01,  2.5092e-01, -4.5677e-01, -4.2561e-02,  2.8006e-03,\n",
      "         3.7374e-02,  1.6416e-01, -2.2456e-01,  2.7450e-01,  4.7592e-02,\n",
      "        -3.4806e-01,  2.7360e-01, -1.4563e-01,  3.4865e-01,  4.4984e-02,\n",
      "        -8.1771e-01, -3.3137e-01,  2.0720e-01,  5.8836e-02, -4.3599e-01,\n",
      "         1.8349e-01, -1.2754e-01, -1.0250e+00,  4.0363e-01,  2.7089e-01,\n",
      "         1.7672e-02,  3.6256e-01, -2.6505e-01, -5.7030e-02, -4.6765e-02,\n",
      "         1.4708e-02,  2.1279e-01, -2.6382e-01, -5.7408e-02,  1.6025e-01,\n",
      "         3.2066e-01,  1.1488e-01, -2.7817e-02,  1.2173e-01,  7.1131e-04,\n",
      "        -1.1232e-01, -1.2315e-01,  1.7121e+00, -6.8540e-02, -3.4107e-01,\n",
      "        -1.7849e-01, -2.2316e-01, -2.3904e-01, -1.1916e-01,  3.3816e-01,\n",
      "        -2.9371e-01, -8.4758e-02,  1.1959e-01,  1.9804e-01,  2.4600e-01,\n",
      "        -5.7793e-02,  1.9444e-01, -1.9247e-02,  2.1092e-01,  6.7033e-02,\n",
      "        -3.0501e-02,  1.4133e-01, -1.9949e-01,  1.8656e-02,  1.7898e-01,\n",
      "        -4.5952e-02,  1.2789e-01,  4.2759e-01, -1.2014e-01, -3.5578e-01,\n",
      "         1.3404e-01, -8.7976e-02,  1.6878e-01,  5.0019e-01,  1.6589e-01,\n",
      "         1.8759e-01, -6.8353e-03, -1.1265e+00,  5.9518e-02,  1.9053e-01,\n",
      "        -2.0253e-01, -3.4982e-01, -3.1377e-01, -4.1868e-02, -2.0715e-01,\n",
      "        -2.1187e-01, -4.9612e-01,  2.0569e-03, -1.8547e-01, -3.8220e-01,\n",
      "        -2.9016e-01, -5.2054e-01, -1.4202e-01, -6.8789e-02, -3.8860e-02,\n",
      "        -1.4047e-01, -3.4747e-01, -3.0316e-03,  2.6822e-02,  2.0068e-01,\n",
      "        -8.3324e-02, -7.1874e-02, -1.4426e+00,  4.7139e-02, -1.2134e-01,\n",
      "        -2.5941e-01, -8.7038e-02, -1.0822e-01,  2.1331e-01,  2.9863e-01,\n",
      "         2.6751e-01,  9.0128e-02, -4.1963e-02,  2.0947e-01,  2.5379e-01,\n",
      "         1.4739e-02,  4.6895e-02, -3.8285e-02, -2.3825e-01,  1.9940e-01,\n",
      "        -6.0672e-02,  1.6371e-01, -1.9067e-01,  1.3911e-01,  5.2795e-01,\n",
      "         2.2725e-01, -1.2495e-02,  3.3307e-01, -2.1319e-01, -1.9596e-01,\n",
      "        -3.4740e-01, -1.6646e-01, -1.1870e-01, -8.0815e-02, -3.2054e-01,\n",
      "         1.9662e-02,  3.1560e-01,  1.3060e-01, -2.1340e-01,  1.4276e-01,\n",
      "        -1.1375e-01,  3.7073e-02,  1.6524e-01, -4.7634e-01, -8.3896e-02,\n",
      "         1.6606e-01, -6.2627e-02,  3.5470e-01, -3.9177e-01, -6.5859e-02,\n",
      "         1.7775e-01,  6.4039e-04, -6.4979e-02, -1.1833e-01, -1.8165e-01,\n",
      "        -2.3934e-01,  1.2512e-01, -4.0247e-01,  1.3627e-01,  2.3719e-01,\n",
      "         3.2092e-01, -5.0543e-02, -1.6985e-01, -1.4448e-01, -6.0538e-02,\n",
      "         3.8299e-01,  8.2055e-02,  2.7890e-01, -1.1322e-01,  1.6739e-01,\n",
      "        -1.5398e-01,  2.2415e-01,  7.3410e-02, -2.1719e-01,  3.7325e-02,\n",
      "        -8.4490e-02,  4.2041e-01,  3.1328e-01, -1.3980e-02,  9.9497e-02,\n",
      "         1.2062e-01, -2.1431e-01,  2.1739e-01,  4.5309e-02, -1.2409e-01,\n",
      "        -3.4940e-01, -2.6247e-01, -1.1825e-01,  3.0799e-01, -1.0392e-01,\n",
      "         7.1889e-01, -4.5179e-02,  1.1320e-01,  1.6392e-01,  9.7894e-02,\n",
      "         1.7502e-01,  2.0060e-01, -5.1403e-02,  2.6274e-01,  9.3554e-02,\n",
      "         1.5532e-01, -1.0148e+00, -1.9526e-02, -6.8844e-02,  1.7837e-01,\n",
      "         2.0607e-01,  9.5063e-01, -2.4712e-01,  4.7176e-01, -3.0736e-01,\n",
      "         8.8023e-02,  1.5369e-01, -2.2031e-02, -1.7996e-01,  2.2588e-01,\n",
      "        -9.8135e-02, -3.1258e-02,  4.7508e-01,  1.6759e-01,  1.6640e-01,\n",
      "        -6.4442e-02,  1.4023e-01, -2.0673e-01, -1.1750e-02,  3.5972e-01,\n",
      "         1.5749e-02, -4.3903e-01, -4.4039e-01,  1.0015e-01,  1.0380e-01,\n",
      "        -3.9931e-01, -6.4724e-02,  3.4515e-02,  2.1346e-01,  3.4952e-01,\n",
      "         4.4677e-01, -1.2224e-01, -9.5582e-02,  1.4576e-01, -2.4187e-01,\n",
      "        -2.1777e-02, -2.7916e-02, -3.1105e-02, -2.6687e-01, -2.9267e-01,\n",
      "        -1.9879e-01, -1.0444e-01,  2.1239e-01, -9.2942e-02, -4.1492e+01,\n",
      "         1.4816e-01, -1.5846e-01,  1.2450e-01,  9.6267e-02,  4.7885e-01,\n",
      "        -6.3906e-01, -4.6144e-02, -1.2328e-01,  2.7562e-01,  5.5675e-02,\n",
      "        -1.5377e-01,  6.7195e-01, -1.6152e-01,  6.5431e-02, -1.6705e-01,\n",
      "        -2.6414e-01, -1.9651e-01,  5.0265e-01, -2.8986e-01,  2.5197e-01,\n",
      "        -3.6631e-01,  4.0955e-02, -2.6761e-01,  1.9458e-02,  4.8281e-02,\n",
      "        -2.2926e-01,  6.1803e-02, -2.1599e-02, -7.7731e-02,  4.2794e-02,\n",
      "        -6.7358e-02, -1.5157e-01, -1.9511e-01,  2.2955e-01, -6.6406e-02,\n",
      "        -1.4078e-01,  6.9713e-01, -8.9885e-02, -9.1645e-02,  7.1710e-01,\n",
      "         2.6154e-01, -5.1115e-02,  2.4776e-02,  1.3396e-01,  4.7257e-02,\n",
      "        -1.2984e-01,  9.6837e-02,  4.2532e-01, -3.0092e-02,  1.8121e-01,\n",
      "        -1.8761e+00,  2.3004e-01,  2.5999e-01, -6.6932e-02, -1.3675e-01,\n",
      "         4.3879e-01,  2.4904e-01, -5.8211e-01, -1.1977e+00, -1.6274e+01,\n",
      "        -2.2012e-01, -2.0233e-01,  1.3121e+00,  1.0442e-01, -2.4561e-01,\n",
      "         2.7168e-01,  1.2551e-01, -2.7369e-02, -2.2116e-02, -4.3441e-01,\n",
      "        -5.6527e-02,  1.1287e-01,  3.0399e-01, -1.1049e-01, -5.0307e-01,\n",
      "         1.5042e-01,  1.9055e-01, -9.7601e-02, -4.4104e-02, -4.0223e-01,\n",
      "         2.9820e-02,  7.6168e-01,  2.3667e-01, -9.8134e-02,  8.8513e-02,\n",
      "         3.2267e-01, -7.5665e-02,  3.2819e-01, -1.0963e-01,  1.4017e-01,\n",
      "         7.4898e-03,  2.9031e-04, -2.7769e-01,  9.0898e-01, -1.8999e-01,\n",
      "         2.7725e-01,  3.4341e-01, -6.7212e-02, -2.1104e-01, -8.7906e-02,\n",
      "        -2.2341e-01, -4.1258e-01,  6.4002e-03, -1.8409e-01,  3.2659e-01,\n",
      "        -5.6046e-03,  2.3656e-01, -1.9733e-01, -1.8297e-01,  7.6005e-02,\n",
      "        -5.2600e-02, -3.0539e-02,  4.7317e-02,  1.7336e-02,  1.1382e-01,\n",
      "         8.7636e+01, -1.9539e-01, -1.3516e-01, -2.8005e-01, -1.0845e-01,\n",
      "         2.8422e-01,  1.0415e-01,  9.3625e-02, -2.8183e-01, -1.7387e-01,\n",
      "        -1.4005e-01, -1.5154e-01,  1.9386e+00,  2.0140e-02, -1.6485e-01,\n",
      "         1.2581e-01,  1.7919e-01,  1.0565e+00,  4.9386e-02,  2.7902e-01,\n",
      "        -2.9884e-01, -2.7476e-01,  1.9512e-01, -4.9094e-01,  3.0174e-02,\n",
      "        -4.9829e-02, -1.2384e-01,  1.2031e-01, -9.5343e-02, -1.8312e-01,\n",
      "         2.3163e-01,  7.8543e-02,  3.1498e-02,  8.3636e-03,  2.3490e-01,\n",
      "        -3.6652e-01,  2.7358e-01, -1.5897e-02, -1.2245e-01, -8.7452e-02,\n",
      "         6.9394e-02,  2.3268e-02, -2.3786e-01,  2.8030e-01,  2.1160e-01,\n",
      "        -1.2176e-01,  1.3629e-01, -1.5120e-02, -1.2551e-01, -7.0907e+00,\n",
      "        -8.9412e-01,  1.1830e+00, -1.6048e-02, -3.7939e-01,  1.6731e-02,\n",
      "         2.4957e-01,  7.8625e-02,  8.5946e-03, -3.6144e-01, -3.0949e-01,\n",
      "         1.3455e-01,  1.4799e-01,  1.1152e-01, -3.4162e-01, -8.1219e-02,\n",
      "        -5.9213e-02,  1.9864e+02, -2.3480e-01, -8.7990e-02,  3.6429e-03,\n",
      "         5.2201e-01, -3.8221e-02,  3.1189e-01,  4.9840e-02,  3.2173e-01,\n",
      "         8.4632e-03, -2.5344e-01,  2.9324e-01, -2.3147e-01, -2.7799e-01,\n",
      "         1.4092e-01,  2.5442e-01,  2.1081e-01, -3.3408e-01, -2.2485e-01,\n",
      "        -4.3857e-01,  1.4241e-02, -5.5038e-02, -2.5244e-02,  4.1615e-02,\n",
      "         2.3840e-01,  1.4455e-01,  2.6509e-03,  8.2612e-02,  3.5907e-02,\n",
      "        -5.4456e-03, -3.8717e-02, -8.4002e-01,  5.7051e-02,  8.3934e-02,\n",
      "        -9.2610e-03,  1.4904e-01,  2.3769e-01, -2.3392e-01, -4.1993e-01,\n",
      "         1.0012e-01,  2.3986e-01,  2.7020e-01, -1.9564e-01,  1.1052e-01,\n",
      "         2.4938e-01,  2.3344e-01,  2.3062e-01, -8.7803e-02, -3.1164e-01,\n",
      "         1.3131e-01,  1.2752e-01, -1.0470e-01, -3.4535e-01, -2.0568e-01,\n",
      "         1.1382e-01,  2.0122e+00,  7.3605e-02,  2.4897e-01, -4.5352e-01,\n",
      "        -9.5971e-04,  1.4032e-01, -3.8319e-02, -1.0442e-01, -6.0383e-01,\n",
      "         1.6943e-02,  2.4613e-02, -2.8421e-01, -1.1809e-01, -1.1957e-01,\n",
      "        -1.1679e-01,  6.4599e-01, -1.1740e-01, -2.9696e-01,  1.1499e-01,\n",
      "         2.0698e-02,  1.3848e-01, -1.7641e-01,  8.2495e-02, -4.0334e-01,\n",
      "         2.4142e-01, -5.0107e-02, -3.8273e-01,  1.5987e-02, -3.1301e-01,\n",
      "        -1.9099e-01, -8.5322e-03, -3.1481e-01,  2.4434e-01, -1.7834e-01,\n",
      "        -1.1602e-01, -1.0724e-01, -1.1039e-02,  5.0938e-01, -1.4828e-01,\n",
      "        -2.0169e-01,  1.4499e-01,  2.8543e-01,  1.2732e-01, -8.1739e-02,\n",
      "        -2.8693e-01, -1.2508e-01, -3.4339e-01,  1.5900e-01, -1.2433e-01,\n",
      "        -1.9731e-01, -4.6279e-02,  2.7916e-01, -3.1428e-01,  5.5393e-01,\n",
      "         2.1381e-01, -2.6144e-01,  4.1537e-02,  4.3684e-01,  6.3552e-02,\n",
      "         2.2352e-01,  8.0508e-02, -2.5648e-01,  1.3404e-01, -2.1278e-01,\n",
      "         1.0425e-01,  1.0314e-01, -1.9512e-01,  1.5070e-01,  2.3278e-01,\n",
      "        -1.8242e-01, -3.5514e-01, -1.0847e-01,  5.4581e-02, -4.7568e-01,\n",
      "         4.0855e-01, -2.1201e-02, -3.4782e+00, -1.6326e-02, -1.3826e-01,\n",
      "        -1.1286e-01, -5.2297e-04, -8.9259e-02,  1.5908e-01,  3.3119e-02,\n",
      "        -7.6041e-01, -2.8092e-01,  4.8603e-02, -2.2134e-01,  2.4552e-01,\n",
      "        -1.4665e-01,  2.5361e-01,  8.2299e-02, -1.5790e-01,  3.7565e-01,\n",
      "         6.5781e-02, -1.8618e-02, -1.5480e-02,  2.5019e-02,  1.0279e-01,\n",
      "        -8.1330e-02,  1.8559e-01,  8.8131e-02,  1.7536e-01, -1.0608e-01,\n",
      "        -3.1083e-01,  2.6105e-01,  5.1849e-02, -2.8929e-02, -1.7680e-01,\n",
      "        -2.7065e-01,  1.0043e-01, -4.0948e-01, -6.6393e-02, -1.6228e-01,\n",
      "         1.4860e-01, -9.7119e-02, -1.9163e-01,  9.9897e-02,  9.5437e-02,\n",
      "         3.1675e-02,  9.9944e-03, -1.1346e-01, -7.8967e-02, -3.5900e-02,\n",
      "         2.1360e-01, -4.8701e-02,  1.4458e-01,  2.3801e-01,  1.6225e+00,\n",
      "         1.3347e-01, -3.0390e-01, -6.5982e-01,  1.6941e-01,  1.1215e-01,\n",
      "         1.4301e-01,  1.2443e-01,  2.2931e-01,  1.3509e-01, -1.5047e-01,\n",
      "         2.0613e-02,  8.2664e-02, -2.5661e-01, -1.1773e-01, -1.4079e-01,\n",
      "         1.7192e-02,  2.8389e-01, -2.3206e-02,  1.1397e-01,  5.0557e-01,\n",
      "        -1.4019e-01,  1.1624e-01, -5.3905e-02, -1.3424e-01, -1.6659e-01,\n",
      "         3.1542e-02, -2.6769e-01, -1.0520e-01, -7.6202e-02, -1.1052e-01,\n",
      "        -5.3684e-03, -5.0547e-02, -1.7180e-01, -2.3234e-01, -2.0976e-02,\n",
      "        -1.0886e-01, -3.0936e-01,  1.9830e-01,  1.5453e+00,  8.0821e-02,\n",
      "         1.2950e-01, -9.7792e-02,  1.1520e-01, -3.9057e-01,  2.3372e-01,\n",
      "         3.7395e-02,  2.9584e-01,  4.4762e-01, -4.0435e-02, -1.2724e-02,\n",
      "         4.9240e-03, -5.3181e-02, -3.2727e-01, -5.8874e-02,  1.3707e-01,\n",
      "         1.5705e-01,  2.4458e-01,  1.4358e-01,  6.2260e-02,  2.9130e-02,\n",
      "        -2.5338e-01, -2.2220e-01,  2.4943e-01,  3.4213e-01, -2.7114e-01,\n",
      "        -4.9780e-01, -6.3034e-01, -5.8355e-02, -3.3800e-01, -4.9334e-02,\n",
      "        -1.4600e-01,  4.4816e-02, -9.1439e-02, -2.7217e-01,  1.1004e-01,\n",
      "        -2.3853e-01, -6.5974e-01, -2.9214e-01,  6.7293e-03,  7.6632e-02,\n",
      "        -2.6102e-01,  2.7159e-01,  3.1902e+00,  1.7952e-01, -6.9729e-02,\n",
      "         9.6386e-02,  1.8497e-01, -4.7129e-02])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "\n",
    "input_ids = tokenizer(text)['input_ids']\n",
    "print(input_ids)\n",
    "input_ids_tensor = torch.tensor([input_ids])\n",
    "# Generate the embeddings for input IDs \n",
    "with torch.no_grad():\n",
    "    model_output = model(input_ids_tensor)\n",
    "    last_hidden_states = model_output.last_hidden_state\n",
    "    \n",
    "# Extract the embeddings for the input IDs from the last hidden layer\n",
    "input_embeddings = last_hidden_states[0,1:-1,:]\n",
    "\n",
    "# compute the average of the embeddings\n",
    "input_embeddings = torch.mean(input_embeddings, dim=0)\n",
    "\n",
    "print(input_embeddings.shape)\n",
    "print(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.6855e-01],\n",
      "         [ 3.7386e-01],\n",
      "         [ 4.6706e-04],\n",
      "         [ 6.0389e-01],\n",
      "         [ 6.4243e-01],\n",
      "         [-3.2507e-01],\n",
      "         [ 3.5675e-01],\n",
      "         [ 2.4668e-01]],\n",
      "\n",
      "        [[ 1.6855e-01],\n",
      "         [-7.1995e-02],\n",
      "         [-2.3302e-01],\n",
      "         [-4.9603e-01],\n",
      "         [ 6.3315e-01],\n",
      "         [-9.9420e-01],\n",
      "         [ 6.0006e-01],\n",
      "         [-2.5515e-01]]])\n",
      "tensor([[[-0.0781],\n",
      "         [-0.2016],\n",
      "         [-0.7156],\n",
      "         [ 0.0527],\n",
      "         [-0.7122],\n",
      "         [ 0.9955],\n",
      "         [-0.3101],\n",
      "         [-0.4686]],\n",
      "\n",
      "        [[ 0.0576],\n",
      "         [ 0.3285],\n",
      "         [ 0.7088],\n",
      "         [ 0.2213],\n",
      "         [ 0.2567],\n",
      "         [-0.1029],\n",
      "         [ 0.1181],\n",
      "         [ 0.6520]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_input_embeddings(texts, model, tokenizer):\n",
    "    # Tokenize the input texts\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Disable gradient calculations\n",
    "    with torch.no_grad():\n",
    "        # Extract token embeddings directly from the embeddings layer\n",
    "        # This retrieves the embeddings after they've been summed with position embeddings\n",
    "        token_embeddings = model.embeddings(input_ids=inputs['input_ids'])\n",
    "        \n",
    "    # Return the embeddings\n",
    "    return token_embeddings\n",
    "\n",
    "def get_bert_embeddings(text, model, tokenizer):\n",
    "    # Tokenize the text\n",
    "    input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    # Get the BERT model embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**input)\n",
    "        embeddings = model_output.last_hidden_state\n",
    "\n",
    "    # Return the embeddings\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Example usage\n",
    "texts = [\"Hello, world!\", \"How are you doing today?\"]\n",
    "input_embeddings = get_input_embeddings(texts, model, tokenizer)\n",
    "\n",
    "print(input_embeddings[:,:,:1])  # This should print: [batch_size, sequence_length, embedding_size]\n",
    "\n",
    "# Get BERT embeddings for a text\n",
    "embeddings = get_bert_embeddings(texts, model, tokenizer)\n",
    "\n",
    "print(embeddings[:,:,:1])  # This should print: [batch_size, sequence_length, hidden_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16222, 19506, 13701]\n",
      "[16222, 19506, 13701]\n",
      "[8752]\n",
      "[8752]\n",
      "Cosine similarity between 'accomodate' and 'accommodate': [[0.5151014]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_word_embedding(text, word, model, tokenizer):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Find the index of the word (handling subword tokenization)\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "    word_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n",
    "    token_ids = inputs['input_ids'][0].tolist()  # Convert to list to handle more easily\n",
    "    # Finding the first complete match of the word tokens in the input IDs\n",
    "    for i in range(len(token_ids)):\n",
    "        if token_ids[i:i+len(word_tokens)] == word_ids:\n",
    "            print(token_ids[i:i+len(word_tokens)])\n",
    "            print(word_ids)\n",
    "            word_index = i\n",
    "            break\n",
    "\n",
    "    # Get the embeddings from BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "\n",
    "    # Extract the embeddings for the word (average subword embeddings if needed)\n",
    "    word_embeddings = embeddings[0, word_index:word_index+len(word_tokens)].mean(dim=0)\n",
    "    \n",
    "    return word_embeddings\n",
    "\n",
    "# Example texts\n",
    "text1 = \"The hotel staff went out of their way to accomodate the guests.\"\n",
    "text2 = \"The hotel staff went out of their way to accommodate the guests.\"\n",
    "\n",
    "# Get embeddings for the specific words\n",
    "embedding1 = get_word_embedding(text1, \"accomodate\", model, tokenizer)\n",
    "embedding2 = get_word_embedding(text2, \"accommodate\", model, tokenizer)\n",
    "\n",
    "# Reshape embeddings to match expected input for cosine_similarity ([1, -1] for single vector)\n",
    "embedding1 = embedding1.unsqueeze(0)\n",
    "embedding2 = embedding2.unsqueeze(0)\n",
    "\n",
    "# Compute the cosine similarity\n",
    "similarity = cosine_similarity(embedding1, embedding2)\n",
    "\n",
    "# Print the similarity\n",
    "print(\"Cosine similarity between 'accomodate' and 'accommodate':\", similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king embedding:  tensor([ 2.7180,  3.1631,  0.3012,  ..., -6.9639, -5.1866,  1.8697])\n",
      "man embedding: tensor([ 2.8015,  3.5800, -0.1190,  ..., -6.7876, -3.8558,  1.8777])\n",
      "woman embedding:  tensor([ 3.0411,  5.3653,  0.3071,  ..., -6.2418, -3.3228,  2.6389])\n",
      "queen embedding: tensor([ 2.5185,  5.2505, -0.6024,  ..., -7.1251, -2.5000,  1.6070])\n",
      "Cosine similarity:  0.9301317930221558\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_id = \"gpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id, output_attentions=True).to(device)\n",
    "model.eval()\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "def get_word_embedding(word, model, tokenizer):\n",
    "    # Encode the word to get token IDs\n",
    "    token_ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "    \n",
    "    # Convert token IDs to tensor and move it to the model's device\n",
    "    tokens_tensor = torch.tensor([token_ids], device=model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model\n",
    "        outputs = model(tokens_tensor)\n",
    "        # Retrieve the hidden states from the model output\n",
    "        hidden_states = outputs[0]  # 'outputs' is a tuple, the first element is the hidden states\n",
    "\n",
    "    # Averaging over the sequence length\n",
    "    return hidden_states[0].mean(dim=0)\n",
    "\n",
    "king_emb = get_word_embedding('I am a jndjsc scndsnc burger', model, tokenizer)\n",
    "man_emb = get_word_embedding('Man', model, tokenizer)\n",
    "woman_emb = get_word_embedding('Woman', model, tokenizer)\n",
    "queen_emb = get_word_embedding('Queen', model, tokenizer)\n",
    "\n",
    "# print all the embeddings\n",
    "print(\"king embedding: \", king_emb)\n",
    "print(\"man embedding:\", man_emb)\n",
    "print(\"woman embedding: \", woman_emb)\n",
    "print(\"queen embedding:\", queen_emb)\n",
    "from torch.nn.functional import cosine_similarity\n",
    "analogy_emb = king_emb - man_emb + woman_emb\n",
    "similarity = cosine_similarity(king_emb.unsqueeze(0), queen_emb.unsqueeze(0))\n",
    "print(\"Cosine similarity: \", similarity.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
